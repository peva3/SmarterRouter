# SmarterRouter Configuration - Environment Configuration Reference
# Copy this file to .env and customize values for your deployment
# All variables are optional - sensible defaults are provided

# =============================================================================
# BACKEND PROVIDER CONFIGURATION
# =============================================================================

# Provider selection: ollama, llama.cpp, or openai (default: ollama)
ROUTER_PROVIDER=ollama

# === Ollama Backend Settings ===
# URL of your Ollama instance
# IMPORTANT: When running SmarterRouter in Docker and Ollama on the host,
# use http://172.17.0.1:11434 instead of http://localhost:11434 because
# localhost inside the container refers to the container itself, not the host.
# If you're running both in Docker Compose, use the service name (e.g., http://ollama:11434).
ROUTER_OLLAMA_URL=http://172.17.0.1:11434

# Model prefix for Ollama (optional)
# Prepends a string to all model names sent to Ollama
# Example: ROUTER_MODEL_PREFIX=myorg/ would make "llama3" become "myorg/llama3"
# Useful for organizational naming or when using a model registry
ROUTER_MODEL_PREFIX=

# Model to keep permanently loaded in VRAM (optional)
# Useful for keeping a small/fast model ready for simple queries
# Example: ROUTER_PINNED_MODEL=phi3:mini
ROUTER_PINNED_MODEL=

# === llama.cpp Server Settings ===
# Uncomment to use llama.cpp server instead of Ollama
# ROUTER_PROVIDER=llama.cpp
# ROUTER_LLAMA_CPP_URL=http://localhost:8080

# Model prefix for llama.cpp (optional)
# Prepends a string to all model names sent to the server
# Example: ROUTER_MODEL_PREFIX=custom/ would make "llama3" become "custom/llama3"
# ROUTER_MODEL_PREFIX=

# === OpenAI-Compatible API Settings ===
# Uncomment to use external OpenAI-compatible API
# ROUTER_PROVIDER=openai
# ROUTER_OPENAI_BASE_URL=https://api.openai.com/v1
# ROUTER_OPENAI_API_KEY=your-api-key-here

# Model prefix for OpenAI-compatible APIs (optional)
# Prepends a string to all model names sent to the API
# Example: ROUTER_MODEL_PREFIX=azure/ would make "gpt-4" become "azure/gpt-4"
# ROUTER_MODEL_PREFIX=

# =============================================================================
# LLM-AS-JUDGE CONFIGURATION
# =============================================================================

# Enable LLM-as-Judge for quality-based profiling (default: false)
# When enabled, a powerful model (the "Judge") grades the output of other models.
ROUTER_JUDGE_ENABLED=false

# Model to use as the Judge (e.g., gpt-4o, mistral-large-latest)
ROUTER_JUDGE_MODEL=gpt-4o

# Base URL for the Judge's API endpoint
ROUTER_JUDGE_BASE_URL=https://api.openai.com/v1

# API Key for the Judge's service
ROUTER_JUDGE_API_KEY=

# Optional: HTTP Referer header (required by some providers like OpenRouter)
# This helps with analytics and rankings on provider platforms
# Example: ROUTER_JUDGE_HTTP_REFERER=https://mysite.com
ROUTER_JUDGE_HTTP_REFERER=

# Optional: X-Title header (required by some providers like OpenRouter)  
# Site/app name for provider analytics
# Example: ROUTER_JUDGE_X_TITLE=SmarterRouter
ROUTER_JUDGE_X_TITLE=

# Retry configuration for transient errors (429 rate limit, 5xx server errors)
# Max retry attempts before falling back to basic scoring
ROUTER_JUDGE_MAX_RETRIES=3

# Initial delay between retries in seconds (doubles each retry: 1s, 2s, 4s...)
ROUTER_JUDGE_RETRY_BASE_DELAY=1.0

# =============================================================================
# EMBEDDINGS CONFIGURATION
# =============================================================================

# Default embedding model (optional)
# Some frontends might not specify a model for /v1/embeddings
# ROUTER_DEFAULT_EMBEDDING_MODEL=all-minilm:latest

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server bind address (default: 0.0.0.0 binds to all interfaces)
# Change to 127.0.0.1 to accept only local connections
ROUTER_HOST=0.0.0.0

# Server port (default: 11434 - matches Ollama default for drop-in replacement)
ROUTER_PORT=11436

# =============================================================================
# EXTERNAL FRONTEND CONFIGURATION
# =============================================================================
# Name the router presents itself as to external UIs (e.g., OpenWebUI)
# This will be the single model name shown in model selection dropdowns.
# Default: SmarterRouter
ROUTER_EXTERNAL_MODEL_NAME=smarterrouter/main

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Admin API key for protecting admin endpoints (REQUIRED FOR PRODUCTION)
# ⚠️  SECURITY WARNING: Leaving this empty makes admin endpoints (/admin/*) publicly accessible!
# In production, you MUST set this to a secure random string.
# 
# Admin endpoints provide:
# - Full model profiles and performance data
# - VRAM monitoring and management
# - Cache invalidation
# - Reprofiling controls
# 
# Example: ROUTER_ADMIN_API_KEY=sk-smarterrouter-$(openssl rand -hex 32)
ROUTER_ADMIN_API_KEY=

# Enable rate limiting (default: false)
# Set to true to prevent abuse and DoS attacks
ROUTER_RATE_LIMIT_ENABLED=false

# Rate limit for general endpoints (requests per minute per client IP)
ROUTER_RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Rate limit for admin endpoints (requests per minute per client IP)
ROUTER_RATE_LIMIT_ADMIN_REQUESTS_PER_MINUTE=10

# =============================================================================
# MODEL ROUTING CONFIGURATION
# =============================================================================

# Router model for LLM-based dispatch (optional)
# Leave empty to use keyword-based routing (fast, no extra LLM calls)
# Set to a small model like "llama3.2:1b" for smarter LLM-based routing
#ROUTER_MODEL=llama3.2:1b

# Dispatcher temperature (default: 0.0 for deterministic routing)
ROUTER_TEMPERATURE=0.0

# Dispatcher max tokens (default: 50)
ROUTER_MAX_TOKENS=50

# Scoring preferences
# Prefer smaller models for simple tasks (default: true)
ROUTER_PREFER_SMALLER_MODELS=true

# Prefer newer models (default: true)
ROUTER_PREFER_NEWER_MODELS=true

# =============================================================================
# TIMEOUT CONFIGURATION
# =============================================================================

# Timeout for profiling operations in seconds (default: 90)
# Lower values speed up profiling but may cause timeouts on large models (14B+)
# Increase this if profiling large models like qwen-14b, deepseek-r1:14b, etc.
ROUTER_PROFILE_TIMEOUT=90

# Timeout for model generation in seconds (default: 120)
# Larger models need more time - increase if large models timeout
ROUTER_GENERATION_TIMEOUT=120

# Number of prompts per category during profiling (default: 3)
# Higher values give more accurate profiles but take longer
ROUTER_PROFILE_PROMPTS_PER_CATEGORY=3

# =============================================================================
# PROFILING WARMUP & LOADING
# =============================================================================

# Warmup timeout calculation: assumes conservative disk read speed (MB/s)
# Used to calculate generous timeouts for the initial model load phase
# Larger models need more time to load from disk into memory
# Default: 50 MB/s (covers slow HDDs and fast SSDs alike)
#ROUTER_PROFILE_WARMUP_DISK_SPEED_MBPS=50

# Maximum warmup timeout in seconds (default: 1800 = 30 minutes)
# Caps the warmup wait time to prevent indefinite hangs on broken models
#ROUTER_PROFILE_WARMUP_MAX_TIMEOUT=1800

# =============================================================================
# ADAPTIVE PROFILING TIMEOUTS
# =============================================================================

# Adaptive timeout automatically adjusts per-model timeouts based on actual
# performance measured during the screening phase. This ensures that:
# - Fast models (phi3:mini, llama3.2:1b) get short timeouts (~30-60s)
# - Slow models (deepseek-r1:7b, reasoning models) get long timeouts (~300-600s)
# - No manual tuning needed regardless of hardware or model mix

# Minimum adaptive timeout in seconds (default: 30s)
# Prevents over-aggressive timeouts that would fail fast models
#ROUTER_PROFILE_ADAPTIVE_TIMEOUT_MIN=30

# Maximum adaptive timeout in seconds (default: 1800 = 30 minutes)
# Caps timeout to prevent indefinite hangs on truly broken models
#ROUTER_PROFILE_ADAPTIVE_TIMEOUT_MAX=1800

# Safety factor for adaptive timeout calculation (default: 2.0 = conservative)
# Higher = more buffer, Lower = more aggressive
# Formula: timeout = max_prompt_time × safety_factor
#ROUTER_PROFILE_ADAPTIVE_SAFETY_FACTOR=2.0

# =============================================================================
# BENCHMARK CONFIGURATION
# =============================================================================

# Benchmark data sources (comma-separated)
# Available: huggingface, lmsys, artificial_analysis
ROUTER_BENCHMARK_SOURCES=huggingface,lmsys

# ArtificialAnalysis.ai settings (optional, requires API key)
# Get your API key from: https://artificialanalysis.ai/insights
#ROUTER_ARTIFICIAL_ANALYSIS_API_KEY=your-api-key-here

# Cache TTL for ArtificialAnalysis data (seconds)
# Free tier has 1000 requests/day, so cache for at least 24h recommended
#ROUTER_ARTIFICIAL_ANALYSIS_CACHE_TTL=86400

# Model mapping file for ArtificialAnalysis
# YAML file mapping AA model IDs/names to SmarterRouter model names
# See artificial_analysis_models.example.yaml for format
#ROUTER_ARTIFICIAL_ANALYSIS_MODEL_MAPPING_FILE=/path/to/mapping.yaml

# =============================================================================
# RESPONSE CONFIGURATION
# =============================================================================

# Enable model signature in responses (default: true)
# Adds "\nModel: {model_name}" to identify which model was used
ROUTER_SIGNATURE_ENABLED=true

# Signature format string
# Use {model} placeholder for model name
ROUTER_SIGNATURE_FORMAT=\nModel: {model}

# =============================================================================
# MONITORING & MAINTENANCE
# =============================================================================

# Polling interval in seconds for checking new models (default: 60)
ROUTER_POLLING_INTERVAL=60

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
ROUTER_LOG_LEVEL=INFO

# Log format: "text" (human-readable) or "json" (structured for log aggregation)
ROUTER_LOG_FORMAT=text

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Database URL for storing profiles and benchmarks (default: SQLite)
# For production, consider PostgreSQL: postgresql://user:pass@localhost/router
# Note: The database file and parent directories are automatically created on startup
ROUTER_DATABASE_URL=sqlite:///data/router.db

# =============================================================================
# SMART CACHE CONFIGURATION
# =============================================================================

# Enable smart caching (default: true)
# Caches routing decisions and responses for faster processing
ROUTER_CACHE_ENABLED=true

# Maximum number of routing cache entries (default: 500)
# Uses exact SHA-256 hash of prompt for instant cache hits
ROUTER_CACHE_MAX_SIZE=500

# Cache TTL in seconds (default: 3600 = 1 hour)
ROUTER_CACHE_TTL_SECONDS=3600

# Similarity threshold for semantic matching (0.0-1.0, default: 0.85)
# Only used if ROUTER_EMBED_MODEL is configured
# Higher = more strict matching, Lower = more flexible matching
ROUTER_CACHE_SIMILARITY_THRESHOLD=0.85

# Maximum number of response cache entries (default: 200)
# Caches full model responses to avoid regeneration
ROUTER_CACHE_RESPONSE_MAX_SIZE=200

# Embedding model for semantic similarity (optional)
# If set, enables semantic similarity matching in addition to exact hash matching
# Allows similar prompts (not just identical) to hit the cache
# Example: ROUTER_EMBED_MODEL=nomic-embed-text:latest
# Note: Exact hash caching works without this setting
ROUTER_EMBED_MODEL=

# =============================================================================
# VRAM MONITORING & MANAGEMENT
# =============================================================================

# Enable VRAM monitoring via nvidia-smi (default: true)
# Provides visibility into GPU memory usage and helps prevent OOM errors
ROUTER_VRAM_MONITOR_ENABLED=true

# Interval for VRAM sampling in seconds (default: 30)
# Lower values give more granular data but slightly more overhead
ROUTER_VRAM_MONITOR_INTERVAL=30

# Maximum VRAM (in GB) the router is allowed to allocate for models
# Set this below your GPU's total memory to leave room for system/overhead
# Example: For a 24GB GPU, set to 22.0 to reserve 2GB
# If not set, will auto-detect GPU total and use 90% of it as default
ROUTER_VRAM_MAX_TOTAL_GB=

# How often to log a VRAM summary to the application log (seconds)
ROUTER_VRAM_LOG_INTERVAL=60

# VRAM Profiling - measure actual memory usage during model profiling
ROUTER_PROFILE_MEASURE_VRAM=true

# Delay after loading a model before measuring VRAM (seconds)
# Allows memory to stabilize after initial allocation
ROUTER_PROFILE_VRAM_SAMPLE_DELAY=2.0

# Number of VRAM samples to take during profiling (averaged)
ROUTER_PROFILE_VRAM_SAMPLES=3

# Auto-unload models when VRAM pressure is high
ROUTER_VRAM_AUTO_UNLOAD_ENABLED=true

# VRAM utilization percentage threshold for warnings in /admin/vram (default: 85%)
# When utilization meets or exceeds this value, a warning is included in the response.
# This does not directly trigger unloads; unloads happen automatically when loading a new model if VRAM is insufficient.
ROUTER_VRAM_UNLOAD_THRESHOLD_PCT=85.0

# Strategy for selecting which models to unload
# "lru" = least recently used (default)
# "largest" = unload biggest models first to free most memory quickly
ROUTER_VRAM_UNLOAD_STRATEGY=lru

# Default VRAM estimate for models that haven't been profiled (GB)
# Used for capacity planning when vram_required_gb is unknown
ROUTER_VRAM_DEFAULT_ESTIMATE_GB=8.0

