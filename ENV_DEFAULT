# LLM Router Configuration - Environment Configuration Reference
# Copy this file to .env and customize values for your deployment
# All variables are optional - sensible defaults are provided

# =============================================================================
# BACKEND PROVIDER CONFIGURATION
# =============================================================================

# Provider selection: ollama, llama.cpp, or openai (default: ollama)
ROUTER_PROVIDER=ollama

# === Ollama Backend Settings ===
# URL of your Ollama instance
ROUTER_OLLAMA_URL=http://localhost:11434

# Model to keep permanently loaded in VRAM (optional)
# Useful for keeping a small/fast model ready for simple queries
# Example: ROUTER_PINNED_MODEL=phi3:mini
ROUTER_PINNED_MODEL=

# === llama.cpp Server Settings ===
# Uncomment to use llama.cpp server instead of Ollama
# ROUTER_PROVIDER=llama.cpp
# ROUTER_LLAMA_CPP_URL=http://localhost:8080

# === OpenAI-Compatible API Settings ===
# Uncomment to use external OpenAI-compatible API
# ROUTER_PROVIDER=openai
# ROUTER_OPENAI_BASE_URL=https://api.openai.com/v1
# ROUTER_OPENAI_API_KEY=your-api-key-here
# ROUTER_MODEL_PREFIX=

# =============================================================================
# LLM-AS-JUDGE CONFIGURATION
# =============================================================================

# Enable LLM-as-Judge for quality-based profiling (default: false)
# When enabled, a powerful model (the "Judge") grades the output of other models.
ROUTER_JUDGE_ENABLED=false

# Model to use as the Judge (e.g., gpt-4o, mistral-large-latest)
ROUTER_JUDGE_MODEL=gpt-4o

# Base URL for the Judge's API endpoint
ROUTER_JUDGE_BASE_URL=https://api.openai.com/v1

# API Key for the Judge's service
ROUTER_JUDGE_API_KEY=

# =============================================================================
# EMBEDDINGS CONFIGURATION
# =============================================================================

# Default embedding model (optional)
# Some frontends might not specify a model for /v1/embeddings
# ROUTER_DEFAULT_EMBEDDING_MODEL=all-minilm:latest

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server bind address (default: 0.0.0.0 binds to all interfaces)
# Change to 127.0.0.1 to accept only local connections
ROUTER_HOST=0.0.0.0

# Server port (default: 11434 - matches Ollama default for drop-in replacement)
ROUTER_PORT=11436

# =============================================================================
# EXTERNAL FRONTEND CONFIGURATION
# =============================================================================
# Name the router presents itself as to external UIs (e.g., OpenWebUI)
# This will be the single model name shown in model selection dropdowns.
# Default: SmarterRouter
ROUTER_EXTERNAL_MODEL_NAME=smarterrouter/main

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Admin API key for protecting admin endpoints (optional but recommended)
# When set, admin endpoints (/admin/*) require Bearer token authentication
# Example: ROUTER_ADMIN_API_KEY=your-secret-key-change-this-in-production
ROUTER_ADMIN_API_KEY=

# Enable rate limiting (default: false)
# Set to true to prevent abuse and DoS attacks
ROUTER_RATE_LIMIT_ENABLED=false

# Rate limit for general endpoints (requests per minute per client IP)
ROUTER_RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Rate limit for admin endpoints (requests per minute per client IP)
ROUTER_RATE_LIMIT_ADMIN_REQUESTS_PER_MINUTE=10

# =============================================================================
# MODEL ROUTING CONFIGURATION
# =============================================================================

# Router model for LLM-based dispatch (optional)
# Leave empty to use keyword-based routing (fast, no extra LLM calls)
# Set to a small model like "llama3.2:1b" for smarter LLM-based routing
#ROUTER_MODEL=llama3.2:1b

# Dispatcher temperature (default: 0.0 for deterministic routing)
ROUTER_TEMPERATURE=0.0

# Dispatcher max tokens (default: 50)
ROUTER_MAX_TOKENS=50

# Scoring preferences
# Prefer smaller models for simple tasks (default: true)
ROUTER_PREFER_SMALLER_MODELS=true

# Prefer newer models (default: true)
ROUTER_PREFER_NEWER_MODELS=true

# =============================================================================
# TIMEOUT CONFIGURATION
# =============================================================================

# Timeout for profiling operations in seconds (default: 30)
# Lower values speed up profiling but may miss slow models
ROUTER_PROFILE_TIMEOUT=30

# Timeout for model generation in seconds (default: 120)
# Larger models need more time - increase if large models timeout
ROUTER_GENERATION_TIMEOUT=120

# Number of prompts per category during profiling (default: 3)
# Higher values give more accurate profiles but take longer
ROUTER_PROFILE_PROMPTS_PER_CATEGORY=3

# =============================================================================
# BENCHMARK CONFIGURATION
# =============================================================================

# Benchmark data sources (comma-separated)
# Available: huggingface, lmsys, artificial_analysis
ROUTER_BENCHMARK_SOURCES=huggingface,lmsys

# =============================================================================
# RESPONSE CONFIGURATION
# =============================================================================

# Enable model signature in responses (default: true)
# Adds "\nModel: {model_name}" to identify which model was used
ROUTER_SIGNATURE_ENABLED=true

# Signature format string
# Use {model} placeholder for model name
ROUTER_SIGNATURE_FORMAT=\nModel: {model}

# =============================================================================
# MONITORING & MAINTENANCE
# =============================================================================

# Polling interval in seconds for checking new models (default: 60)
ROUTER_POLLING_INTERVAL=60

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
ROUTER_LOG_LEVEL=INFO

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Database URL for storing profiles and benchmarks (default: SQLite)
# For production, consider PostgreSQL: postgresql://user:pass@localhost/router
ROUTER_DATABASE_URL=sqlite:///router.db

# =============================================================================
# SMART CACHE CONFIGURATION
# =============================================================================

# Enable smart caching (default: true)
# Caches routing decisions and responses for faster processing
ROUTER_CACHE_ENABLED=true

# Maximum number of routing cache entries (default: 500)
# Uses exact SHA-256 hash of prompt for instant cache hits
ROUTER_CACHE_MAX_SIZE=500

# Cache TTL in seconds (default: 3600 = 1 hour)
ROUTER_CACHE_TTL_SECONDS=3600

# Similarity threshold for semantic matching (0.0-1.0, default: 0.85)
# Only used if ROUTER_EMBED_MODEL is configured
# Higher = more strict matching, Lower = more flexible matching
ROUTER_CACHE_SIMILARITY_THRESHOLD=0.85

# Maximum number of response cache entries (default: 200)
# Caches full model responses to avoid regeneration
ROUTER_CACHE_RESPONSE_MAX_SIZE=200

# Embedding model for semantic similarity (optional)
# If set, enables semantic similarity matching in addition to exact hash matching
# Allows similar prompts (not just identical) to hit the cache
# Example: ROUTER_EMBED_MODEL=nomic-embed-text:latest
# Note: Exact hash caching works without this setting
ROUTER_EMBED_MODEL=

# =============================================================================
# VRAM MONITORING & MANAGEMENT
# =============================================================================

# Enable VRAM monitoring via nvidia-smi (default: true)
# Provides visibility into GPU memory usage and helps prevent OOM errors
ROUTER_VRAM_MONITOR_ENABLED=true

# Interval for VRAM sampling in seconds (default: 30)
# Lower values give more granular data but slightly more overhead
ROUTER_VRAM_MONITOR_INTERVAL=30

# Maximum VRAM (in GB) the router is allowed to allocate for models
# Set this below your GPU's total memory to leave room for system/overhead
# Example: For a 24GB GPU, set to 22.0 to reserve 2GB
# If not set, will auto-detect GPU total and use 90% of it as default
ROUTER_VRAM_MAX_TOTAL_GB=

# How often to log a VRAM summary to the application log (seconds)
ROUTER_VRAM_LOG_INTERVAL=60

# VRAM Profiling - measure actual memory usage during model profiling
ROUTER_PROFILE_MEASURE_VRAM=true

# Delay after loading a model before measuring VRAM (seconds)
# Allows memory to stabilize after initial allocation
ROUTER_PROFILE_VRAM_SAMPLE_DELAY=2.0

# Number of VRAM samples to take during profiling (averaged)
ROUTER_PROFILE_VRAM_SAMPLES=3

# Auto-unload models when VRAM pressure is high
ROUTER_VRAM_AUTO_UNLOAD_ENABLED=true

# VRAM utilization percentage threshold for warnings in /admin/vram (default: 85%)
# When utilization meets or exceeds this value, a warning is included in the response.
# This does not directly trigger unloads; unloads happen automatically when loading a new model if VRAM is insufficient.
ROUTER_VRAM_UNLOAD_THRESHOLD_PCT=85.0

# Strategy for selecting which models to unload
# "lru" = least recently used (default)
# "largest" = unload biggest models first to free most memory quickly
ROUTER_VRAM_UNLOAD_STRATEGY=lru

# Default VRAM estimate for models that haven't been profiled (GB)
# Used for capacity planning when vram_required_gb is unknown
ROUTER_VRAM_DEFAULT_ESTIMATE_GB=8.0

